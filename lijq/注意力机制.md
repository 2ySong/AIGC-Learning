---
tags:
  - 深度学习
  - 《UnderstandingDeepLearning》
draft: true
---
如何处理大文本数据？

"The restaurant refused to serve me a ham sandwich, because it only cooks vegetarian food. In the end, they just gave me two slices of bread. Their ambience was just as good as the food and service."

1. 编码输入可能出乎意料的大。（每个英文单词可能由长度 1024 的嵌入向量表示，即便一篇短文其编码输入长度也达到 10 万数量级）
2. [[NLP 自然语言处理|NLP问题]] 的一个定义性特征是每个输入（）的长度不同。网络应该在不同输入位置的单词间共享参数。
3. 语言是模糊的；前者应该“关注”后者（如后出现的代词 it 指代前文的 restaurant），且链接的强度取决于单词本身；有时这些链接需要跨越大段文本。

> [!info]+ 《动手学深度学习》中关于 query/key/value 的说明 
>  
> 自主性的与非自主性的注意力提示解释了人类的注意力的方式，下面来看看如何通过这两种注意力提示，用神经网络来设计注意力机制的框架。  
> 
> 首先，考虑一个相对简单的状况，即只使用非自主性提示。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大汇聚层或平均汇聚层。因此，**“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来**。  
> 
> ![](https://zh-v2.d2l.ai/_images/qkv.svg)
> 在注意力机制中，自主性提示被称为**查询**（query），而非自主性提示（客观存在的咖啡杯和书本）作为**键**（key）与感官输入（sensory inputs）的**值**（value）构成一组 pair 作为输入。而给定任何查询，注意力机制通过**注意力汇聚**（attention pooling）将非自主性提示的 key 引导至感官输入。


一种常见的网络架构——自注意力模型（self-attention model）

![[../../PHOTO/深度学习/注意力机制/img-20251106-1412-51349.png|600]]
![[../../PHOTO/深度学习/注意力机制/img-20251106-1415-30521.png|600]]
![[../../PHOTO/深度学习/注意力机制/img-20251106-1416-51106.png|600]]
## 应用场景
**输入是一组向量，并且输入的向量的数量会改变，即每次输入模型的向量序列的长度都不一样**
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23425.png|300]]
【场景假设】
- 考虑文字处理，将句子中每个词描述成向量且序列大小不定
	- 独热编码
		需要假设所有的词彼此之间是没有关系的，无法观察语义信息
	- 词嵌入（word embedding）
		![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23426.png|350]]
- 声音处理
	划分帧与步长，每段语音信号看做一个向量
- 图向量（如社交网络，药物发现）
	适合采用独热编码

#### 类型1：输入与输出数量相同
词性标注/语音识别/网络节点特性归类
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23428.png|400]]
#### 类型2：输入是一个序列，输出是一个标签
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23427.png|400]]
#### 类型3：序列到序列任务
不知道应该输出多少个标签，机器需要自己决定输出多少个标签。如语音识别
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23436.png|400]]
## 运作原理
*图中使用FC代表全连接网络*
考虑输入和输出数量一样多的状况，以序列标注（sequence labeling）为例
1. 直觉使用全连接网络把每一个向量分别输入全连接网络，得到输出
2. 输入是同一个词，则无法输出不同的内容——>添加上下文信息
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23460.png|400]]
3. 使用窗口考虑上下文信息
	需要考虑整个序列的话——>窗口大，过拟合；用自注意力模型考虑输入序列信息
	![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23461.png|400]]
	> 自注意力模型不是只能用一次，而是可以叠加很多次，全连接网络和自注意力模型交替使用
	> [[1706.03762] Attention Is All You Need](https://arxiv.org/abs/1706.03762)提出了Transformer网络架构. 其中最重要的模块是自注意力模块

【计算自注意力】
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23462.png|400]]
其中 $a$ 为处理后的输入，$b$ 为输出向量
1. 计算向量间的关联程度：计算点积
$$
q=W^qa^i,k=W^ka^j,\alpha_{i,j}=q\cdot k
$$

> 还有其他的计算方式，比如相加
2. 查询-键-值（Query-Key-Value，QKV）模式
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23463.png|400]]
一般在实践的时候，$a^1$ 跟自身也有关联性，$\alpha_{1,1}=(W^qa^1)\cdot (W^k a^1)$
3. 对所有的关联性执行softmax操作
$$
\alpha_{1,i}^{^{\prime}}=\frac{\exp\left(\alpha_{1,i}\right)}{\sum_j\exp\left(\alpha_{1,j}\right)}
$$
> 不一定要用softmax函数，也可以用别的激活函数，比如ReLU.

4. 根据关联性（即注意力分数）抽取重要的信息
$$
b^1=\sum_i\alpha_{1,i}^{^{\prime}}v^i
$$
其中 $v^i=W^va^i$
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23464.png|400]]
【矩阵乘法角度理解】
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23466.png|400]]
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23465.png|400]]
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23467.png|400]]
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23468.png|400]]
其中 $A'$ 称作自注意力矩阵；自注意力层中需要学习的参数仅 $W^{q},W^{k}, W^{v}$

## 多头注意力
多头自注意力（multi-head self-attention）是自注意力的高级版本。因为相关有很多种不同的形式，也许可以有多个 $q$，不同的 $q$ 负责不同种类的相关性
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23468 1.png|400]]
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23471.png|400]]
## 位置编码
在实现自注意力的时候，如果觉得位置信息很重要，就要用到位置编码（positional encoding）
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23472.png|400]]
位置编码可通过各种不同的方法来生成。目前还不知道哪一种方法最好，这是一个尚待研究的问题
## 截断自注意力
可以处理向量序列过长的问题。这个范围是人为设定的。在做语音识别的时候，如果要辨识某个位置有什么样的音标，以及这个位置有什么的内容也许没必要让自注意力考虑整个句子，以提高运算的速度

## 与 CNN/RNN/GNN 的关系
【与 CNN 的对比】
在做卷积神经网络的时候，只考虑感受野范围内的信息；而自注意力会考虑整幅图像的信息，可以看作一种简化版的自注意力

![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23473.png|400]]
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23471 1.png|400]]
> 更多参考 [[1911.03584] On the Relationship between Self-Attention and Convolutional Layers](https://arxiv.org/abs/1911.03584)

【与 RNN 对比】
- 循环神经网络中的隐向量存储了历史信息，可以看作一种记忆（memory）
- 自注意力的每一个向量都考虑了整个输入序列，而循环神经网络的每一个向量只考虑左边已经输入的向量，而没有考虑右边的向量。但循环神经网络也可以是双向的，如果使用Bi-RNN，则每一个隐状态的输出也可以看作考虑了整个输入序列
- 对于循环神经网络，如果最右边黄色的向量要考虑最左边的输入，则必须把最左边的输入存到记忆里才能不被遗忘，并且直至带到最右边，才能够在最后一个时间点被考虑，但只要自注意力模型输出的查询和键匹配，自注意力模型就可以轻易地从整个序列上非常远的向量中抽取信息
- RNN 无法并行
![[../../PHOTO/深度学习/注意力机制/img-20251106-1403-23474.png|500]]
> 更多参考[[2006.16236] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/abs/2006.16236)

【与 GNN 关系】
- 图中不仅有节点的信息，还有边（edge）的信息，用于表示某些节点间是有关联的
- 关联性不需要机器自动找出来，所以当把自注意力用在图上面的时候，只计算有边相连的节点
- 当把自注意力按照这种限制用在图上面的时候，其实使用的就是一种图神经网络